---
layout: single
title: "[Paper review] All in One: Multi-Task Prompting for Graph Neural Network (KDD 2023)"
---
# All in One: Multi-Task Prompting for Graph Neural Network
### ë…¼ë¬¸ì„ ì½ê²Œ ëœ ê³„ê¸°

ìµœê·¼ LLMì˜ ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì€ë°, Graphë¡œ pre-trained modelì„ fine-tuningí•˜ì—¬ downstream taskì—ì„œì˜ í™œìš©í•˜ê¸° ìœ„í•œ ìµœê·¼ ì—°êµ¬ê°€ ê¶ê¸ˆí•´ì„œ ì½ìŒ. 


> ğŸ’¡ **`3ì¤„ ìš”ì•½`**
> 
> GNNì—ì„œì˜ ê¸°ì¡´ ì§€ë„í•™ìŠµì´ ê°–ëŠ” ë°ì´í„° ë¼ë²¨ ë¶€ì¡±ê³¼ ê³¼ì í•© ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ NLPì—ì„œ ì“°ì´ëŠ” pre-training, fine-tuning í”„ë ˆì„ì›Œí¬ë¥¼ GNNì— ì ìš©í•˜ì—¬ pre-trainingìœ¼ë¡œ ì¼ë°˜ì  ì§€ì‹ í•™ìŠµì„ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ë ¤ëŠ” ì‹œë„ê°€ ì—°êµ¬ë˜ê³  ìˆë‹¤. 
>
> í•˜ì§€ë§Œ, downstream taskê°€ ë  ìˆ˜ ìˆëŠ” node level, edge, level, graph level taskê°€ ê°ê° ë„ˆë¬´ ìƒì´í•˜ì—¬ pre-training ëª¨ë¸ì˜ ë§¥ë½ê³¼ í˜¸í™˜ë˜ì§€ ì•Šì„ ê²½ìš°ê°€ ìˆë‹¤.
> 
> ë³¸ ë…¼ë¬¸ì—ì„œëŠ”, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ â€œ**pretraining and fine-tuning**â€ êµ¬ì¡°ë¥¼ â€œ**pre-training, prompting, and fine-tuning**â€ìœ¼ë¡œ í™•ì¥í•œ êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ë©°, ë§ì€ ì‹¤í—˜ì„ í†µí•´ í•´ë‹¹ êµ¬ì¡°ì˜ íš¨ê³¼ë¥¼ ì¦ëª…í•œë‹¤.

### Terminologies

> **Pre-training**
NLPì™€ ê·¸ë˜í”„ í•™ìŠµì—ì„œ ë§ì´ ì‚¬ìš©ë˜ë©°, ëª¨ë¸ì„ ë¹„ì§€ë„ í•™ìŠµì„ í†µí•´ `**í° ë°ì´í„°ì…‹ì— ëŒ€í•´ í›ˆë ¨í•˜ëŠ” ê²ƒ**`ì„ ë§í•œë‹¤.
> 

> **Fine-tuning**
Fine-tuningì€ ì‚¬ì „ í›ˆë ¨ ì´í›„ì— ì´ì–´ì§€ëŠ” ë‹¨ê³„ë¡œ, ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ íŠ¹ì • í•˜ìœ„ ì‘ì—…ì˜ `**ë ˆì´ë¸”ì´ ìˆëŠ” ì‘ì€ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¶”ê°€ì ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ê²ƒ**`ì„ ì˜ë¯¸í•œë‹¤.
> 

> **Prompting**
Prompting ê¸°ë²•ì€ `**ëª¨ë¸ì˜ ë™ì‘ì„ Fine-tuning ê³¼ì •ì—ì„œ íŠ¹ì • ë°©í–¥ìœ¼ë¡œ ê°€ì´ë“œí•˜ê¸° ìœ„í•´ ì…ë ¥ ë°ì´í„°ë¥¼ ì¬ì •ì˜í•˜ëŠ” ê¸°ìˆ **`ì´ë‹¤.
ì´ ê¸°ë²•ì€ ì…ë ¥ ë°ì´í„°ì— í”„ë¡¬í”„íŠ¸ í† í° ë˜ëŠ” ì§€ì‹œì‚¬í•­ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ í–‰ë™ì„ íŠ¹ì • ì‘ì—… ë°©í–¥ìœ¼ë¡œ ì´ëˆë‹¤.
NLPì—ì„œ í”„ë¡¬í”„íŠ¸ëŠ” **ì¼ë°˜ì ìœ¼ë¡œ ì…ë ¥ í…ìŠ¤íŠ¸ì— ë¶€ì°©ë˜ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ êµ¬ë¬¸ì´ë‚˜ ë²¡í„°**ë¡œ, ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…(ì˜ˆ: ì§ˆë¬¸ ë‹µë³€, ê°ì„± ë¶„ë¥˜)ìœ¼ë¡œ ì´ë„ëŠ” ì—­í• ì„ í•œë‹¤.
> 

### Background

- **Graph Neural Networks**
    - **The nature of most GNNs is to capture the underlying message-passing patterns** for graph representation.
    - Recent works also consider how to make graph learning more adaptive when data
    annotation is insufficient or how to transfer the model to a new domain, which triggered many graph pre-training studies instead of traditional supervised learning.
- **Graph Pre-training.**
    - **node-level:** node classification(ex. GCA)
    - **edge-level:** edge prediction
    - **graph-level:** contrastive learning (ex. GraphCL, SimGRACE)
        - Contrastive learning & augmentation: self-supervised learning ê¸°ë²• ([ì°¸ê³  ë§í¬](https://www.blossominkyung.com/deeplearning/contrastive-learning))
        - GraphCL minimizes the distance between a pair of graph-level representations for the same graph with different augmentations whereas SimGRACE tries to perturb the graph model parameter spaces and narrow down the gap between different perturbations for the same graph.
- **Prompt Learning & Motivations**
    - graph level pre-training strategies have some intrinsic similarities with the language-masked prediction task: `**aligning two graph views generated by node/edge/feature mask or other perturbations**` is very similar to **predicting some vacant â€œblanksâ€ on graphs**. That inspires us to further consider: why canâ€™t we use a similar format prompt for graphs to improve the generalization of graph neural networks?
    - Instead of fine-tuning a pre-trained model with an adaptive task head, **`prompt learning aims to reformulate input data to fit the pretext`**

### Introduction

- Challenges in designing graph prompt
    - Language Promptë³´ë‹¤ ë³µì¡í•¨
    - downstream taskì™€ pre-trained ëª¨ë¸ ê°„ ê°„ê·¹ í•´ê²°
    - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” promptë¥¼ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€
- Ideas
    - ì–¸ì–´, ê·¸ë˜í”„ í”„ë¡¬í”„íŠ¸ í˜•ì‹ í†µì¼(section 3.3).
        
        1) prompt tokens, 2) token structures ë° 3) inserting patternsë¡œ **`ê·¸ë˜í”„ í”„ë¡¬í”„íŠ¸ ì„¤ê³„`**
        
    - graph-level taskë¡œ ì¬ì •ì˜í•˜ëŠ” ë°©ë²• ì œì•ˆ (section 3.2).
        
        `**Original graphì—ì„œ ìœ ë„ëœ ê·¸ë˜í”„ì— ì˜í•´ node-level ë° edge-level taskì„ graph-level taskë¡œ ì¬ì •ì˜í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆ**`
        
    - graph prompt ****ì—°êµ¬ì— meta-learning ê¸°ìˆ  ë„ì… (section 3.4).
        
        `**Multi-taskë¥¼ ìœ„í•´ meta-learning ê¸°ìˆ ì„ ë„ì…í•˜ì—¬ reliable í”„ë¡¬í”„íŠ¸ë¥¼ í•™ìŠµ**`
        

### 3. Multi-task prompting on graphs

**3.1. Overview of our framework**

**ëª©í‘œ**: ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **`ì›ë³¸ ê·¸ë˜í”„ì— ì‚½ì…í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ ê·¸ë˜í”„ë¥¼ í•™ìŠµ`**í•¨ìœ¼ë¡œì¨, ê·¸ë˜í”„ í”„ë¦¬íŠ¸ë ˆì´ë‹ ì „ëµê³¼ ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ê°„ì˜ ê²©ì°¨ë¥¼ ë” ì¢íˆê³ , **ì‚¬ì „ ì§€ì‹ì„ ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ì–´ë ¤ì›€ì„ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œ**ë¡œ í•©ë‹ˆë‹¤.

Objective: In this paper, we aim to learn a prompt graph that can be inserted into the original graph, through which we wish to further bridge the gap between a graph pre-training strategy and multiple downstream tasks, and further relieve the difficulties of transferring prior knowledge to different domains.

**ê°œìš”**: ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ê·¸ë˜í”„ ëª¨ë¸ì— ëŒ€í•œ ìƒˆë¡œìš´ ë©€í‹°íƒœìŠ¤í¬ í”„ë¡¬í”„íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. **ì²«ì§¸**, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ê·¸ë˜í”„ ì‘ì—…ì„ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ í†µí•©í•˜ê³  ì´ëŸ¬í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ë“¤ì„ ê·¸ë˜í”„ ìˆ˜ì¤€ ì‘ì—…ìœ¼ë¡œ ì¬ì •ì˜ (section 3.2)í•©ë‹ˆë‹¤. **ë‘˜ì§¸**, í†µí•©ëœ ê·¸ë˜í”„ ìˆ˜ì¤€ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬, í•™ìŠµ ê°€ëŠ¥í•œ í† í°, ë‚´ë¶€ êµ¬ì¡° ë° ì ì‘í˜• ì‚½ì… íŒ¨í„´ì„ ê°€ì§„ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ ê·¸ë˜í”„ (section 3.3)ë¥¼ í†µí•´ ë‹¤ì¤‘ ì‘ì—… ê°„ì˜ ê²©ì°¨ë¥¼ ë” ì¤„ì…ë‹ˆë‹¤. **ì…‹ì§¸**, ë©€í‹°íƒœìŠ¤í¬ ì„¤ì •ì—ì„œ ë” ì ì‘ì ì¸ ê·¸ë˜í”„ í”„ë¡¬í”„íŠ¸ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ë©”íƒ€ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤ (section 3.4)ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒìœ¼ë¡œ, ì£¼ìš” êµ¬ì„± ìš”ì†Œë“¤ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ í”„ë ˆì„ì›Œí¬ëŠ” ê·¸ë˜í”„ í•™ìŠµ ë¶„ì•¼ì—ì„œì˜ í”„ë¡¬í”„íŠ¸ ë””ìì¸ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì¼ë°˜í™”ì— ê¸°ì—¬í•˜ë©°, ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ì‚¬ì „ ì§€ì‹ì„ ì „ë‹¬í•˜ëŠ”ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤

Overview: To achieve our goal, we propose a novel multi-task prompting framework for graph models. First, we unify various graph tasks in the same format and reformulate these downstream
tasks as graph-level tasks. Second, with the unified graph-level instances, we further narrow down the gap among multiple tasks by a novel prompt graph with learnable tokens, inner structures, and

adaptive inserting patterns. Third, we build a meta-learning process to learn more adaptive graph prompts for multi-task settings. Next, we elaborate on the main components.

**3.2. Reformulating Downstream Tasks**

![Untitled](/assets/images/r001/Untitled.png)

**3.2.1. Why Reformulate Downstream Tasks.** 

NLPì—ì„œ â€œpre-training and fine-tuningâ€ frameworkê°€ ì„±ê³µì ì¸ ì´ìœ ëŠ” pre-training taskì™€ downstream taskê°€ ê³µí†µì ì¸ ë³¸ì§ˆ taskë¥¼ ê³µìœ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

í•˜ì§€ë§Œ, GNNì˜ node-level taskì™€ edge-level taskëŠ” ë§¤ìš° ë‹¤ë¥¸ ì‘ì—…ìœ¼ë¡œ, ë§¤ìš° ì‘ì€ ê³µí†µì  ë³¸ì§ˆì„ ê³µìœ í•œë‹¤.  â†’ ë¶€ì •ì  ì „ì´ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆë‹¤.

**3.2.2 Why Reformulate to the Graph Level.**

ê·¸ë˜í”„ì—ì„œì˜ ì ì¬ì  task spaceë¥¼ ì¬ê²€í† í•˜ê³  Figure 3bì—ì„œ ë³´ì—¬ì§€ëŠ” ê³„ì¸µì  ê´€ê³„ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

node-level (node feature ë³€ê²½, node ì¶”ê°€/ì‚­ì œ), edge-level (edge ì¶”ê°€/ì‚­ì œ) â†’ ê¸°ë³¸ì ì¸ graph level ì—°ì‚°ìœ¼ë¡œ ë‹¤ë£° ìˆ˜ ìˆë‹¤.

graph-level taskê°€ ê°€ì¥ ì¼ë°˜ì ì´ë©° ë§ì€ ê²¹ì¹˜ëŠ” ì§€ì‹ ì „ì´ì— ëŒ€í•œ sub-spaceë¥¼ ì°¨ì§€í•œë‹¤.

**3.2.3. How to Reformulate Downstream Tasks.**

node-level taskì™€ edge-level taskë¥¼ ê°ê° ë…¸ë“œì™€ ì—£ì§€ì— ëŒ€í•œ induced ê·¸ë˜í”„ë¡œ ì¬ì •ì˜

![Untitled](/assets/images/r001/Untitled%201.png)

Figure 4a: target nodeì— ëŒ€í•œ induced ê·¸ë˜í”„ëŠ” ë„¤íŠ¸ì›Œí¬ì—ì„œ í•´ë‹¹ ë…¸ë“œì˜ ğœ ê±°ë¦¬ ì´ë‚´ì˜ local ì˜ì—­

Figure 4b: ë…¸ë“œ ìŒì— ëŒ€í•œ induced ê·¸ë˜í”„ (ë…¸ë“œ ìŒì€ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©´ ì–‘ì˜ ì—£ì§€ë¡œ ì·¨ê¸‰ë˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìŒì˜ ì—£ì§€ë¡œ ì·¨ê¸‰) 

ì´ í•˜ìœ„ê·¸ë˜í”„ëŠ” ì´ ë…¸ë“œ ìŒì„ ê·¸ë“¤ì˜ ğœ ê±°ë¦¬ ì´ë‚´ ì´ì›ƒìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì‰½ê²Œ êµ¬ì„±

edge-level taskì„ graph labelë¡œì„œ ëŒ€ìƒ ë…¸ë“œ ìŒì˜ ì—£ì§€ ë ˆì´ë¸”ê³¼ í•¨ê»˜ ì¬ì •ì˜

ë¬´ë°©í–¥ì„± ê·¸ë˜í”„ì˜ ê²½ìš°, ğœ ê±°ë¦¬ëŠ” ğœ-í™‰ ê¸¸ì´ì™€ ê°™ìŠµë‹ˆë‹¤. 

ê°€ì¤‘ ê·¸ë˜í”„ì˜ ê²½ìš°, ğœ ê±°ë¦¬ëŠ” ìµœë‹¨ ê²½ë¡œ ê±°ë¦¬ë¥¼ ì˜ë¯¸í•˜ë©°, ì—¬ê¸°ì„œ induced ê·¸ë˜í”„ëŠ” ì—¬ëŸ¬ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ [1, 39]ì„ í†µí•´ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**3.3 Prompt Graph Design**

**3.3.1 Prompting NLP and Graph in One Way.**

NLPì™€ ê·¸ë˜í”„ ì˜ì—­ì˜ í”„ë¡¬í”„íŠ¸ëŠ” ì ì–´ë„ ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤: 

(1) **í”„ë¡¬í”„íŠ¸ í† í°ì€ ì…ë ¥ ë‹¨ì–´/ë…¸ë“œ ë²¡í„°ì™€ ë™ì¼í•œ í¬ê¸°ì˜ ë²¡í„°í™”ëœ í”„ë¡¬í”„íŠ¸ ì •ë³´ë¥¼ í¬í•¨**í•©ë‹ˆë‹¤;

(2) **í† í° êµ¬ì¡°**ëŠ” **ì„œë¡œ ë‹¤ë¥¸ í† í°ë“¤ì˜ ì—°ê²°**ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. NLP ì˜ì—­ì—ì„œ í”„ë¡¬í”„íŠ¸ í† í°(ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ë‹¨ì–´)ì€ í•˜ìœ„ ë¬¸ì¥ ë˜ëŠ” êµ¬ì ˆê³¼ ê°™ì´ ì„ í˜•ì ì¸ ê´€ê³„ë¡œ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, ê·¸ë˜í”„ ì˜ì—­ì—ì„œëŠ” ì„œë¡œ ë‹¤ë¥¸ í† í°ë“¤ì˜ ì—°ê²°ì€ ë¹„ì„ í˜•ì ì´ë©° NLP í”„ë¡¬í”„íŠ¸ë³´ë‹¤ í›¨ì”¬ ë³µì¡í•©ë‹ˆë‹¤; 

(3) **ì‚½ì… íŒ¨í„´**ì€ **í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ ë°ì´í„°ì— ì–´ë–»ê²Œ ì¶”ê°€í• ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤**. NLP ì˜ì—­ì—ì„œëŠ” í”„ë¡¬í”„íŠ¸ê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì…ë ¥ ë¬¸ì¥ì˜ ì• ë˜ëŠ” ë’¤ì— ì¶”ê°€ë˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ë˜í”„ ì˜ì—­ì—ì„œëŠ” ë¬¸ì¥ê³¼ ê°™ì€ ëª…ì‹œì ì¸ ìœ„ì¹˜ê°€ ì—†ê¸° ë•Œë¬¸ì— ê·¸ë˜í”„ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ë” ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.

**3.3.2 Prompt Tokens.**

ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ G = (V, E)ë¡œ í‘œí˜„í•˜ê³ , ì—¬ê¸°ì„œ V = {ğ‘£1, ğ‘£2, Â· Â· Â· , ğ‘£ğ‘ }ì€ ğ‘ê°œì˜ ë…¸ë“œë¥¼ í¬í•¨í•˜ëŠ” ë…¸ë“œ ì§‘í•©ì´ë©°, ê° ë…¸ë“œëŠ” xğ‘– âˆˆ R1Ã—ğ‘‘ë¡œ í‘œí˜„ë˜ëŠ” íŠ¹ì§• ë²¡í„°ë¥¼ ê°–ìŠµë‹ˆë‹¤. E = {(ğ‘£ğ‘–, ğ‘£ğ‘—)|ğ‘£ğ‘–, ğ‘£ğ‘— âˆˆ V}ì€ ê° ì—£ì§€ê°€ V ë‚´ ë…¸ë“œ ìŒì„ ì—°ê²°í•˜ëŠ” ì—£ì§€ ì§‘í•©ì…ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” í”„ë¡¬í”„íŠ¸ ê·¸ë˜í”„ë¥¼ Gğ‘ = (P, S)ë¡œ í‘œí˜„í•˜ë©°, ì—¬ê¸°ì„œ P = {ğ‘1, ğ‘2, Â· Â· Â· , ğ‘|P|}ëŠ” í”„ë¡¬í”„íŠ¸ í† í°ì˜ ì§‘í•©ì„ ë‚˜íƒ€ë‚´ë©°, |P|ëŠ” í† í°ì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ê° í† í° ğ‘ğ‘– âˆˆ PëŠ” í¬ê¸°ê°€ ì…ë ¥ ê·¸ë˜í”„ì˜ ë…¸ë“œ íŠ¹ì§•ê³¼ ë™ì¼í•œ í¬ê¸°ì˜ í† í° ë²¡í„° pğ‘– âˆˆ R1Ã—ğ‘‘ë¡œ í‘œí˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ, ì¼ë°˜ì ìœ¼ë¡œ |P| â‰ª ğ‘ê³¼ |P| â‰ª ğ‘‘â„ì¸ë°, ì´ ë•Œ ğ‘‘â„ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ê·¸ë˜í”„ ëª¨ë¸ì˜ ì€ë‹‰ì¸µ í¬ê¸°ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í† í° ë²¡í„°ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ê·¸ë˜í”„ì— ğ‘—ë²ˆì§¸ í† í°ì„ ë”í•˜ì—¬ ì…ë ¥ ê·¸ë˜í”„ë¥¼ ì¬ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: xË†ğ‘– = xğ‘– + pğ‘—). ê·¸ëŸ° ë‹¤ìŒ, ìš°ë¦¬ëŠ” í”„ë¡¬í”„íŠ¸ íŠ¹ì§•ìœ¼ë¡œ ì…ë ¥ íŠ¹ì§•ì„ ëŒ€ì²´í•˜ê³ , ì´ë¥¼ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ì¶”ê°€ì ì¸ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**3.3.3 Token Structures.**

S = {(ğ‘ğ‘–, ğ‘ğ‘—)|ğ‘ğ‘–, ğ‘ğ‘— âˆˆ P}ì€ í† í° ê°„ì˜ **ìŒë³„ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í† í° êµ¬ì¡°**ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. NLP í”„ë¡¬í”„íŠ¸ì™€ ë‹¬ë¦¬, í”„ë¡¬í”„íŠ¸ ê·¸ë˜í”„ì—ì„œì˜ í† í° êµ¬ì¡°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì•”ì‹œì ì…ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì„¸ ê°€ì§€ ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ í”„ë¡¬í”„íŠ¸ í† í° êµ¬ì¡°ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤: (1) ì²« ë²ˆì§¸ ë°©ë²•ì€ **ì¡°ì • ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤:
A = |P|âˆ’1 âˆª ğ‘–=1 ğ‘—=ğ‘–+1 {ğ‘ğ‘–ğ‘—} ì´ë•Œ ğ‘ğ‘–ğ‘—ëŠ” í† í° ğ‘ğ‘–ì™€ í† í° ğ‘ğ‘—ê°€ ì—°ê²°ë˜ì–´ì•¼ í•  ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì¡°ì • ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤; (2) **ë‘ ë²ˆì§¸ ë°©ë²•ì€ ê° í”„ë¡¬í”„íŠ¸ í† í° ìŒì˜ ë‚´ì ì„ ì‚¬ìš©í•˜ê³ , ë‚´ì  ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ìŒì„ ì œê±°í•©ë‹ˆë‹¤**. ì´ ê²½ìš°ì—ëŠ” (ğ‘ğ‘–, ğ‘ğ‘—) âˆˆ Sê°€ ğœ(pğ‘– Â· pğ‘—) < ğ›¿ì¸ ê²½ìš°ì…ë‹ˆë‹¤. ì´ë•Œ ğœ(Â·)ì€ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì´ê³ , ğ›¿ì€ ì‚¬ì „ ì •ì˜ëœ ì„ê³„ê°’ì…ë‹ˆë‹¤; (3) ì„¸ ë²ˆì§¸ ë°©ë²•ì€ **í† í°ì„ ë…ë¦½ì ìœ¼ë¡œ ì·¨ê¸‰**í•˜ê³ , ê·¸ë˜ì„œ S = âˆ…ê°€ ë©ë‹ˆë‹¤.

**3.3.4 Inserting Patterns**

ğœ“ëŠ” í”„ë¡¬í”„íŠ¸ ê·¸ë˜í”„ Gğ‘ì„ ì…ë ¥ ê·¸ë˜í”„ Gì— ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ë‚˜íƒ€ë‚´ëŠ” ì‚½ì… í•¨ìˆ˜ì…ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì¡°ì‘ëœ ê·¸ë˜í”„ëŠ” Gğ‘š = ğœ“ (G, Gğ‘)ë¡œ í‘œì‹œë©ë‹ˆë‹¤. `**ìš°ë¦¬ëŠ” ì‚½ì… íŒ¨í„´ì„ í”„ë¡¬í”„íŠ¸ í† í°ê³¼ ì…ë ¥ ê·¸ë˜í”„ ë…¸ë“œ ê°„ì˜ ë‚´ì ìœ¼ë¡œ ì •ì˜**`í•˜ê³ , ê·¸ëŸ° ë‹¤ìŒ ê°€ì¤‘ì¹˜ ê°’ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì •í•œ ì—°ê²°ì„ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, xË†ğ‘– = xğ‘– + âˆ‘|P|ğ‘˜=1ğ‘¤ğ‘–ğ‘˜pğ‘˜ì™€ ê°™ì´ ì¡°ì‘ëœ ë…¸ë“œ xË†ğ‘–ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ğ‘¤ğ‘–ğ‘˜ëŠ” ë¶ˆí•„ìš”í•œ ì—°ê²°ì„ ì œê±°í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ê°’ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤:

**3.4 Multi-task Prompting via Meta Learning**

**3.4.1 Constructing Meta Prompting Tasks.**

supporting data, query dataë¥¼ ì§€ì •í•œë‹¤.

graph-level: labeled graph í¬í•¨

node-level: ê° nodeì— ëŒ€í•´ induced graphë¥¼ ìƒì„±í•˜ì—¬ graph labelì„ target node labelê³¼ í˜¸í™˜í•œë‹¤.

edge-level:  train, testìš© edge induced graphë¥¼ ê°ê° ìƒì„±í•œë‹¤. edge labelì€ í•´ë‹¹ edgeì˜ ë‘ ëì ì— ë”°ë¼ ê²°ì •

**3.4.2 Applying Meta-learning to Graph Prompting**

ğœƒ : prompt parameters, ğœ‹âˆ— : fixed parameters of the pre-trained graph backbone, ğœ™ : taskerâ€™s parameters, ğ‘“ğœƒ,ğœ™ |ğœ‹âˆ— : pipeline, $L_{D} (ğ‘“ )$ : task loss with pipeline f on data D

ë§¤ task $\tau_i$ ì—ì„œ ëŒ€ì‘í•˜ëŠ” parameterëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ ëœë‹¤.

![Untitled](/assets/images/r001/Untitled%202.png)

where the initialization is set as: $\theta_{i}^{0}= \theta, \phi_{i}^{0}= \phi$

ëª©í‘œ: meta prompting taskë¥¼ ìœ„í•´ íš¨ê³¼ì ì¸ ì´ˆê¸°ê°’ $(\theta, \phi)$ë¥¼ ì–»ëŠ” ê²ƒ

ë°©ë²•: ë‹¤ì–‘í•œ taskì˜ **`meta loss`**ë¥¼ ìµœì†Œí™”

![Untitled](/assets/images/r001/Untitled%203.png)

chain ruleì— ì˜í•´ second-order gradientë¥¼ ì´ìš©í•´ query dataë¥¼ ë°”íƒ•ìœ¼ë¡œ $\theta(í˜¹ì€ \phi)$ë¥¼ ê°±ì‹ í•œë‹¤.

: Model-Agnostic Meta-Learning(MAML) framework ì‚¬ìš© - ë°œì „ëœ First order MAMLë„ ìˆìŒ.(ì„±ëŠ¥ ê±°ì˜ ë¹„ìŠ·)

chain rule: $\left\{f(g(x))\right\}'=f'(g(x))\times g'(x)$

First-order gradient: loss functionì— ëŒ€í•œ íŒŒë¼ë¯¸í„°ì˜ ë³€í™”ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°

second-order gradient: **ì†ì‹¤ í•¨ìˆ˜ì˜ ê³¡ë¥  ì •ë³´**ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. 

â†’ meta loss: task lossë¥¼ minimizeí•˜ê¸° ìœ„í•œ loss â†’ task lossê°€ ê·¹ì†Ÿê°’ì„ ê°–ëŠ” ì§€ì ì„ second-order gradientë¥¼ ì´ìš©í•´ ì°¾ì„ ìˆ˜ ìˆë‹¤.

![Untitled](/assets/images/r001/Untitled%204.png)

![Untitled](/assets/images/r001/Untitled%205.png)

**3.4.3 Overall Learning Process.**

í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ ë‹¤ì–‘í•œ graph taskë¥¼ multi-task episodeë¡œ êµ¬ì„±í•˜ê³ ,. node/edge/graph classë¥¼ binary classificationìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë™ì¼í•œ task headë¥¼ ê³µìœ í•˜ë„ë¡ í•œë‹¤.

![Untitled](/assets/images/r001/Untitled%206.png)

**3.5 Why It Works?**

**3.5.1 Connection to Existing Work.** 

- GPPT
    - graph prompt ì—°êµ¬
    - edge-predictionì„ pre-trainingì˜ pretextë¡œ ì‚¬ìš©í•˜ê³ , node classificationì„ downstream taskë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ original graphì— ì¶”ê°€í•˜ëŠ” (prompt) labeled tokenì„ ì„¤ê³„í•¨.
- ë³¸ ì—°êµ¬ì™€ ì°¨ì´ì 
    - GPPT is not flexible to manipulate original graphs
    - GPPT is only applicable for node classification
    - GPPT only supports edge prediction task as the pretext but is not compatible with more
    advanced graph-level pre-training strategies such as GraphCL [36], UGRAPHEMB [2], SimGRACE [35] etc.

**3.5.2 Flexibility**

í”„ë¡¬í”„íŒ…ì˜ ë³¸ì§ˆì€ **ì…ë ¥ ë°ì´í„°ë¥¼ ì‚¬ì „ í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ë„ë¡ ì¡°ì‘**í•˜ëŠ” ê²ƒ

**data operationì˜ flexibity**ëŠ” í”„ë¡¬í”„íŒ… ì„±ëŠ¥ì˜ ë³‘ëª©ì´ë‹¤.

ğ‘”ë¥¼ graph-level ë³€í™˜ìœ¼ë¡œ ì •ì˜ (such as â€œchanging node featuresâ€, â€œadding or removing edges/subgraphsâ€ etc.) í•˜ì—¬ **ëª¨ë“  ê·¸ë˜í”„ Gì— ëŒ€í•´ ì ì ˆí•œ prompt token** $p*$ì„ í•™ìŠµí•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì •ì‹ì´ ì„±ë¦½í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆë‹¤. Fang et al. [6]

![Untitled](/assets/images/r001/Untitled%207.png)

ì¦‰, ì›ë³¸ ê·¸ë˜í”„ì— ì ìš©ë  ì ì ˆí•œ í† í°ì„ í•™ìŠµí•˜ì—¬ ëª¨ë“  graph manipulationì„ ëª¨ë°©í•  ìˆ˜ ìˆë‹¤.

ğ‘‚ğ‘ğœ‘: original graphì™€ prompted graphì˜ pre-trained ëª¨ë¸ í‘œí˜„ ê°„ì˜ ì˜¤ì°¨ í•œê³„ 

â†’ëª¨ë¸ì˜ ì¼ë¶€ ë¹„ì„ í˜• ë ˆì´ì–´(unchangeable)ì™€ í•™ìŠµëœ í”„ë¡¬í”„íŠ¸(changable)ì˜ í’ˆì§ˆì— ì˜í–¥ì„ ë°›ìœ¼ë©°, ë°œì „ëœ í”„ë¡¬í”„íŠ¸ ë°©ë²•ìœ¼ë¡œ ë”ìš± ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ë…¼ë¬¸ì—ì„œëŠ” `**ë…ë¦½ì ì¸ í† í°ì„ í”„ë¡¬í”„íŠ¸ ê·¸ë˜í”„ë¡œ í™•ì¥í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•œ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ê°–ëŠ” ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ í† í°ì„ í¬í•¨í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆ**`í•©ë‹ˆë‹¤. 

ì‚½ì… í•¨ìˆ˜: ğœ“ (G, Gğ‘), G: original graph, Gğ‘: prompt graph, Gâˆ—ğ‘: an optimal prompt graph 

íš¨ìœ¨ì ì¸ Gâˆ—ğ‘ íŠœë‹ì„ í†µí•´ ìƒˆë¡œìš´ ì˜¤ì°¨ í•œê³„ ğ‘‚âˆ—ğ‘ğœ‘ë¥¼ ë”ìš± ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

![Untitled](/assets/images/r001/Untitled%208.png)

ì„¹ì…˜ 4.6ì—ì„œëŠ” íš¨ê³¼ì ì¸ í•™ìŠµì„ í†µí•´ ğ‘‚âˆ—ğ‘ğœ‘ê°€ ğ‘‚ğ‘ğœ‘ë³´ë‹¤ ìƒë‹¹íˆ ì‘ì„ ìˆ˜ ìˆìŒì„ ê²½í—˜ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ê²ƒì€ ìš°ë¦¬ì˜ ë°©ë²•ì´ ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ ì „ëµê³¼ ì¼ì¹˜í•˜ë„ë¡ ê·¸ë˜í”„ì— ë” ìœ ì—°í•œ ë³€í™˜ì„ ì§€ì›í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**3.5.3 Efficiency.**

**we only need to tune the prompt with the pre-trained graph model frozen**, making the training process converge faster than traditional transfer tuning.

For the graph features and structures, traditional methods usually need to feed the whole graph into a graph model, which needs huge memory to cache these contents.

ì‹¤ì œë¡œ ë§ì€ ì‹¤ì œ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œëŠ” ì „ì²´ ë…¸ë“œ ì¤‘ ì¼ë¶€ì—ë§Œ ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ìš°ë¦¬ì˜ ë°©ë²•ì€ í•„ìš”í•œ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ì ì‹œì— ë©ˆì¶œ ìˆ˜ ìˆìœ¼ë©° ì „ì²´ ê·¸ë˜í”„ì—ì„œ ë©”ì‹œì§€ë¥¼ ì „íŒŒí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì— íŠ¹íˆ ë„ì›€ì´ ë©ë‹ˆë‹¤.

**3.5.4 Compatibility.**

GPPT(prompt model) vs our model

GPPTë³´ë‹¤ ë” ë§ì€ caseì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

 Pre-training with fine-tuning vs our model

When transferring the model to different tasks, traditional approaches usually need to **additionally tune a task head.** In contrast, our method **focuses on the input data manipulation** and it relies less on the downstream tasks. This means we have a larger tolerance for the task head

### 4. Evaluation

ì´ ì„¹ì…˜ì—ì„œëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì„ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²•ë“¤ê³¼ ë…¸ë“œ ìˆ˜ì¤€, ì—£ì§€ ìˆ˜ì¤€, ê·¸ë¦¬ê³  ê·¸ë˜í”„ ìˆ˜ì¤€ ì‘ì—…ì— ëŒ€í•´ í¬ê´„ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. íŠ¹íˆ, ë‹¤ìŒê³¼ ê°™ì€ ì—°êµ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ê³ ì í•©ë‹ˆë‹¤

**4.1.2 Approaches.** 

Compared approaches are from three categories:
**(1) Supervised methods**: these methods directly train a GNN model on a specific task and then directly infer the result. We here take three famous GNN models including GAT [32], GCN [34],
and Graph Transformer [25] (short as GT). These GNN models are also included as the backbones of our prompt methods. 

**(2) Pre-training with fine-tuning**: These methods first pre-train a GNN model in a **self-supervised** way such as GraphCL [36] and SimGRACE [35], then **the pre-trained model will be fine-tuned for
a new downstream task.** 

**(3) Prompt methods:** **With a pre-trained model frozen and a learnable prompt graph**, our prompt method aims to change the input graph and reformulate the downstream task to fit the pre-training strategies.

**Q1: How effective is our method under the few-shot learning background for multiple graph tasks?**

We repeat the evaluation 5 times and report the average results in Table 2, Table 12 (Appendix A), and Table 13 (Appendix A). 

Compared with pre-training approaches, our solutions further improve the compatibility of graph models. The reported improvements range from 1.10% to 8.81% on node-level tasks, 1.28% to 12.26% on edge-level tasks, and 0.14% to 10.77% on graph-level tasks

From the results, we can observe that most supervised methods are very hard to achieve better performance compared with pre-train methods and prompt methods

**GPPT (**50% labeled**) vs Our Method (**100 labeled samples.**)**

 This different setting makes our labeled ratio approximately only 25% on Cora, 18% on CiteSeer, 1.7% on Reddit, 7.3% on Amazon, and 1.5% on Pubmed, which are far less than the reported GPPT 

**Q2: How adaptable is our method when transferred to other domains or tasks?**

(1) how effectively is the model transferred to **different tasks within the same domain**? 

![Untitled](/assets/images/r001/Untitled%209.png)

1) our prompt method significantly outperforms the other approaches and the prediction results make sense. In contrast, the problem of the hard transfer method is that the source model sometimes can not well decide on the target tasks because the target classes may be far away from the source classes.

2) the graph-level task has better adaptability than the node-level task for the edge-level target, which is in line with our previous intuition presented in Figure 3 (section 3.2).

(2) how effectively is the model transferred to **different domains**?

We also conduct the model on Amazon and PubMed as source domains, then load the model states from these source domains and report the performance on the target domain (Cora)

![Untitled](/assets/images/r001/Untitled%2010.png)

In Table 4  good transferability of our prompt also exists when we deal with different domains

**Q3: How do the main components of our method impact the performance?: Ablation study**

In this section, we compare our complete framework with four variants: â€œw/o metaâ€ is the prompt method without meta-learning step; â€œw/o hâ€ is our method without task head tuning, which is
previously introduced in section 3.5.4; â€œw/o token structureâ€ is the prompt where all the tokens are treated as isolated without any inner connection; and â€œw/o insertingâ€ is the prompt without any across links between prompt tokens and the input graphs

í”„ë¡¬í”„íŠ¸ ê·¸ë˜í”„ì™€ ì…ë ¥ ê·¸ë˜í”„ ì‚¬ì´ì˜ ì‚½ì… íŒ¨í„´ì€ ìµœì¢… ì„±ëŠ¥ì— ë§¤ìš° ì¤‘ìš”í•œ ì—­í• 

ì¡°ì • ê°€ëŠ¥í•œ ì‘ì—… í—¤ë“œë¥¼ ì™„ì „íˆ ì œê±°í•´ë„ "w/o h" ë³€í˜•ì€ ì—¬ì „íˆ ë§¤ìš° ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ìƒë¥˜ ì‘ì—…ê³¼ í•˜ë¥˜ ì‘ì—… ì‚¬ì´ë¥¼ ì—°ê²°í•˜ëŠ” ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ì‹œì‚¬

![Untitled](/assets/images/r001/Untitled%2011.png)

**Q4: How efficient is our model compared with traditional approaches?**

![Untitled](/assets/images/r001/Untitled%2012.png)

![Untitled](/assets/images/r001/Untitled%2013.png)

![Untitled](/assets/images/r001/Untitled%2014.png)

**Q5: How powerful is our method when we manipulate graphs?**

we calculate the error bound mentioned in Equation 5 and 6. 

We compare the original error with the naive prompt mentioned in Equation 5, and our prompt graph with 3, 5, and 10 tokens.

As shown in Table 6, our designed prompt graph significantly reduces the error between the original graph and the manipulated graph.

![Untitled](/assets/images/r001/Untitled%2015.png)

As shown in Figure 8, the graph representations from a pre-trained model present lower resolution to node classes compared with our prompted graph.

![Untitled](/assets/images/r001/Untitled%2016.png)

### 5. Conclusion

We propose a novel method to **`reformulate different-level tasks to unified ones and further design an effective prompt graph with a meta-learning technique.`**
---
layout: single
title: "[Paper review] All in One: Multi-Task Prompting for Graph Neural Network (KDD 2023)"
---
# All in One: Multi-Task Prompting for Graph Neural Network
<aside>
💡 **`3줄 요약`**

GNN에서의 기존 지도학습이 갖는 데이터 부족과 과적합 문제를 해결하기 위해 NLP에서 쓰이는 pre-training, fine-tuning 프레임워크를 GNN에 적용하여 pre-training으로 일반적 지식 학습을 수행하여 모델을 초기화하려는 시도가 연구되고 있다. 

하지만, downstream task가 될 수 있는 node level, edge, level, graph level task가 각각 너무 상이하여 pretraining 모델의 맥락과 호환되지 않을 경우가 있다.

본 논문에서는, 이를 해결하기 위해 “pretraining and fine-tuning” 구조를 “pre-training, **prompting**, and finetuning”으로 확장한 구조를 제안하며, 많은 실험을 통해 해당 구조의 효과를 증명한다.

</aside>

### Terminology

> Pre-training
NLP와 그래프 학습에서 많이 사용되며, 모델을 비지도 학습 작업을 통해 큰 데이터셋에 대해 훈련하는 것을 말한다. 
Pre-training의 목표는 데이터로부터 일반적인 패턴과 표현을 학습하여, 이후 특정한 하위 작업에 적용하고 미세 조정할 수 있도록 하는 것이다. 
NLP에서 사전 훈련은 주로 마스크 처리된 언어 모델링을 사용하고, 이 때 모델은 문장에서 누락된 단어를 예측하는 방식으로 훈련한다. 그래프 학습에서는 사전 훈련 작업으로 그래프 내에서 마스크 처리된 노드 또는 엣지를 예측하는 방식을 사용하기도 한다.
> 

> Fine-tuning
Fine-tuning은 사전 훈련 이후에 이어지는 단계로, 사전 훈련된 모델을 특정 하위 작업의 레이블이 있는 작은 데이터셋에 대해 추가적으로 훈련하는 것을 의미한다.
모델은 사전 훈련에서 학습한 지식을 유지하면서 레이블이 있는 데이터를 사용하여 새로운 하위 작업에 적응한다.
> 

> Prompting
Prompting 기법은 모델의 동작을 Fine-tuning 과정에서 특정 방향으로 가이드하기 위해 입력 데이터를 재정의하는 기술이다.
이 기법은 입력 데이터에 프롬프트 토큰 또는 지시사항을 추가하여 모델의 행동을 특정 작업 방향으로 이끈다.
NLP에서 프롬프트는 일반적으로 입력 텍스트에 부착되는 미리 정의된 구문이나 벡터로, 모델을 특정 작업(예: 질문 답변, 감성 분류)으로 이끄는 역할을 한다.
`**그래프 학습에서는 프롬프트 기법을 사용하여 원래 그래프에 삽입할 수 있는 프롬프트 그래프를 생성하여 여러 작업과 도메인에서 모델의 일반화를 돕는다.**`
>

---
layout: single
title: "[Paper review] All in One: Multi-Task Prompting for Graph Neural Network (KDD 2023)"
---
# All in One: Multi-Task Prompting for Graph Neural Network
> **3줄 요약**
>
> GNN에서의 기존 지도학습이 갖는 데이터 부족과 과적합 문제를 해결하기 위해 NLP에서 쓰이는 pre-training, fine-tuning 프레임워크를 GNN에 적용하여 pre-training으로 일반적 지식 학습을 수행하여 모델을 초기화하려는 시도가 연구되고 있다. 
>
> 하지만, downstream task가 될 수 있는 node level, edge, level, graph level task가 각각 너무 상이하여 pretraining 모델의 맥락과 호환되지 않을 경우가 있다.
>
> 본 논문은, 이를 해결하기 위해 **“pretraining and fine-tuning”** 구조를 “**pre-training, prompting, and finetuning**”으로 확장한 구조를 제안하며, 많은 실험을 통해 해당 구조의 효과를 증명한다.
>

